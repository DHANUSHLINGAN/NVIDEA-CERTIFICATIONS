{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated codebases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA codebases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your codebase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (by `CTRL` + clicking them). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a `qdrep` report file which can be used in a variety of manners. We use the `--stats=true` flag here to indicate we would like summary statistics printed. There is quite a lot of information printed:\n",
    "\n",
    "- Profile configuration details\n",
    "- Report file(s) generation details\n",
    "- **CUDA API Statistics**\n",
    "- **CUDA Kernel Statistics**\n",
    "- **CUDA Memory Operation Statistics (time and size)**\n",
    "- OS Runtime API Statistics\n",
    "\n",
    "In this lab you will primarily be using the 3 sections in **bold** above. In the next lab, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-8c67-20b3-2670-ca78.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-8c67-20b3-2670-ca78.qdrep\"\n",
      "Exporting 4555 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-8c67-20b3-2670-ca78.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   91.2      2355051355           1    2355051355.0      2355051355      2355051355  cudaDeviceSynchronize                                                           \n",
      "    8.1       209025118           3      69675039.3           19016       208949254  cudaMallocManaged                                                               \n",
      "    0.7        18320757           3       6106919.0         5532697         7209133  cudaFree                                                                        \n",
      "    0.0           51139           1         51139.0           51139           51139  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0      2355040139           1    2355040139.0      2355040139      2355040139  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.7        68474432        2304         29719.8            1887          181504  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.3        20848224         768         27146.1            1119          159712  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   59.3      5367096046         276      19446000.2           24910       100135674  poll                                                                            \n",
      "   39.6      3581449203         275      13023451.6           10576       100072621  sem_timedwait                                                                   \n",
      "    0.9        80424110         591        136081.4            1004        17839861  ioctl                                                                           \n",
      "    0.2        20535481          90        228172.0            1207         7149235  mmap                                                                            \n",
      "    0.0          649234          77          8431.6            2307           23269  open64                                                                          \n",
      "    0.0          138147           4         34536.8           24212           39287  pthread_create                                                                  \n",
      "    0.0          111293          23          4838.8            1173           19460  fopen                                                                           \n",
      "    0.0          103103           3         34367.7           27993           43238  fgets                                                                           \n",
      "    0.0           80592          11          7326.5            3856           11996  write                                                                           \n",
      "    0.0           42657          13          3281.3            1503            5126  munmap                                                                          \n",
      "    0.0           42447          16          2652.9            1019           14907  fclose                                                                          \n",
      "    0.0           41125           5          8225.0            2948           13251  open                                                                            \n",
      "    0.0           30447          13          2342.1            1040            4874  read                                                                            \n",
      "    0.0           15685           2          7842.5            5929            9756  socket                                                                          \n",
      "    0.0           15449           3          5149.7            4486            5816  pipe2                                                                           \n",
      "    0.0            8491           5          1698.2            1009            3961  fcntl                                                                           \n",
      "    0.0            8374           4          2093.5            1489            3231  mprotect                                                                        \n",
      "    0.0            7618           1          7618.0            7618            7618  connect                                                                         \n",
      "    0.0            7052           2          3526.0            3363            3689  fread                                                                           \n",
      "    0.0            3001           1          3001.0            3001            3001  bind                                                                            \n",
      "    0.0            2072           1          2072.0            2072            2072  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report1.qdrep\"\n",
      "Report file moved to \"/dli/task/report1.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide th `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-7348-ae8a-4e2c-0768.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-7348-ae8a-4e2c-0768.qdrep\"\n",
      "Exporting 4551 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-7348-ae8a-4e2c-0768.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   91.5      2357606359           1    2357606359.0      2357606359      2357606359  cudaDeviceSynchronize                                                           \n",
      "    7.8       202051088           3      67350362.7           20845       201971107  cudaMallocManaged                                                               \n",
      "    0.7        18309421           3       6103140.3         5532084         7147218  cudaFree                                                                        \n",
      "    0.0           59170           1         59170.0           59170           59170  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0      2357594982           1    2357594982.0      2357594982      2357594982  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.6        68435072        2304         29702.7            1887          181824  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.4        20853056         768         27152.4            1119          164032  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   59.4      5368953534         276      19452730.2           16779       100132860  poll                                                                            \n",
      "   39.5      3576508359         276      12958363.6            9974       100067461  sem_timedwait                                                                   \n",
      "    0.9        77247936         589        131151.0            1076        15628699  ioctl                                                                           \n",
      "    0.2        20226131          89        227259.9            1133         7084606  mmap                                                                            \n",
      "    0.0          637676          77          8281.5            2328           25874  open64                                                                          \n",
      "    0.0          112101           4         28025.3           22939           31742  pthread_create                                                                  \n",
      "    0.0          101983          23          4434.0            1189           14980  fopen                                                                           \n",
      "    0.0           94107           3         31369.0           23998           43720  fgets                                                                           \n",
      "    0.0           77265          11          7024.1            3894           12606  write                                                                           \n",
      "    0.0           41977          14          2998.4            1248            4238  munmap                                                                          \n",
      "    0.0           34712           5          6942.4            2833            9030  open                                                                            \n",
      "    0.0           29372          16          1835.8            1025            3518  fclose                                                                          \n",
      "    0.0           24718          12          2059.8            1013            3089  read                                                                            \n",
      "    0.0           13081           2          6540.5            5146            7935  socket                                                                          \n",
      "    0.0           12229           3          4076.3            3666            4543  pipe2                                                                           \n",
      "    0.0            6387           1          6387.0            6387            6387  connect                                                                         \n",
      "    0.0            6130           4          1532.5            1509            1595  mprotect                                                                        \n",
      "    0.0            6057           2          3028.5            2755            3302  fread                                                                           \n",
      "    0.0            5850           3          1950.0            1058            3532  fcntl                                                                           \n",
      "    0.0            2249           1          2249.0            2249            2249  bind                                                                            \n",
      "    0.0            1374           1          1374.0            1374            1374  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report3.qdrep\"\n",
      "Report file moved to \"/dli/task/report3.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimzation you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-e6c8-4dc3-e5c9-f87a.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-e6c8-4dc3-e5c9-f87a.qdrep\"\n",
      "Exporting 4225 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-e6c8-4dc3-e5c9-f87a.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   73.6       625663859           1     625663859.0       625663859       625663859  cudaDeviceSynchronize                                                           \n",
      "   24.2       205930396           3      68643465.3           19136       205840473  cudaMallocManaged                                                               \n",
      "    2.2        18301087           3       6100362.3         5440987         7223522  cudaFree                                                                        \n",
      "    0.0           48485           1         48485.0           48485           48485  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       625652992           1     625652992.0       625652992       625652992  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.7        68579424        2304         29765.4            1727          182560  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.3        20869120         768         27173.3            1119          163872  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   56.4      2127556339         115      18500489.9           19595       100149407  poll                                                                            \n",
      "   40.9      1543113408         115      13418377.5           10348       100067089  sem_timedwait                                                                   \n",
      "    2.1        80147527         586        136770.5            1048        15707275  ioctl                                                                           \n",
      "    0.5        20179395          90        224215.5            1112         7167659  mmap                                                                            \n",
      "    0.0          581522          77          7552.2            2269           18377  open64                                                                          \n",
      "    0.0          130875           4         32718.8           24335           37819  pthread_create                                                                  \n",
      "    0.0          110356          23          4798.1            1192           19273  fopen                                                                           \n",
      "    0.0          103273           3         34424.3           24964           44361  fgets                                                                           \n",
      "    0.0           73993          11          6726.6            3652            9665  write                                                                           \n",
      "    0.0           41246          13          3172.8            1218            4344  munmap                                                                          \n",
      "    0.0           38365           5          7673.0            3177           11295  open                                                                            \n",
      "    0.0           30316          16          1894.7            1026            3981  fclose                                                                          \n",
      "    0.0           23793          12          1982.8            1038            3268  read                                                                            \n",
      "    0.0           16078           2          8039.0            7902            8176  socket                                                                          \n",
      "    0.0           12380           3          4126.7            3572            4632  pipe2                                                                           \n",
      "    0.0            7043           1          7043.0            7043            7043  connect                                                                         \n",
      "    0.0            6690           2          3345.0            3159            3531  fread                                                                           \n",
      "    0.0            6471           4          1617.8            1410            1890  mprotect                                                                        \n",
      "    0.0            5407           2          2703.5            1273            4134  fcntl                                                                           \n",
      "    0.0            2958           1          2958.0            2958            2958  bind                                                                            \n",
      "    0.0            1524           1          1524.0            1524            1524  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report7.qdrep\"\n",
      "Report file moved to \"/dli/task/report7.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQTzaK1iaFkxgYxaxR5QgHCVx1ZqhpX2F3q9UU6sGKCYaNIq6CGAo8W_qyzg2qwpeiZoHd7NCug7OTj/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQTzaK1iaFkxgYxaxR5QgHCVx1ZqhpX2F3q9UU6sGKCYaNIq6CGAo8W_qyzg2qwpeiZoHd7NCug7OTj/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a codebase. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 40\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 5\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaulate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-5e0e-b8c7-d2a2-d559.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-5e0e-b8c7-d2a2-d559.qdrep\"\n",
      "Exporting 4222 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-5e0e-b8c7-d2a2-d559.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   73.5       622439874           1     622439874.0       622439874       622439874  cudaDeviceSynchronize                                                           \n",
      "   24.4       206585545           3      68861848.3           17692       206530840  cudaMallocManaged                                                               \n",
      "    2.2        18251938           3       6083979.3         5543608         7093764  cudaFree                                                                        \n",
      "    0.0           50145           1         50145.0           50145           50145  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       622428078           1     622428078.0       622428078       622428078  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.7        68622880        2304         29784.2            1887          182368  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.3        20861248         768         27163.1            1151          159648  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   56.2      2126905577         115      18494831.1           19956       100129141  poll                                                                            \n",
      "   41.1      1557048645         115      13539553.4           10189       100067889  sem_timedwait                                                                   \n",
      "    2.1        81247393         586        138647.4            1008        15691921  ioctl                                                                           \n",
      "    0.5        20167744          90        224086.0            1145         7038536  mmap                                                                            \n",
      "    0.0          562203          77          7301.3            2633           12357  open64                                                                          \n",
      "    0.0          107347           4         26836.8           21899           31809  pthread_create                                                                  \n",
      "    0.0          101940           3         33980.0           21349           57106  fgets                                                                           \n",
      "    0.0          100691          23          4377.9            1580           13841  fopen                                                                           \n",
      "    0.0           71105          11          6464.1            3792            9322  write                                                                           \n",
      "    0.0           34639          13          2664.5            1002            3905  munmap                                                                          \n",
      "    0.0           33366           5          6673.2            3325           10234  open                                                                            \n",
      "    0.0           30916          16          1932.3            1152            2911  fclose                                                                          \n",
      "    0.0           21459          10          2145.9            1048            2813  read                                                                            \n",
      "    0.0           10568           3          3522.7            2812            4093  pipe2                                                                           \n",
      "    0.0            8134           2          4067.0            3523            4611  socket                                                                          \n",
      "    0.0            6968           4          1742.0            1532            2085  mprotect                                                                        \n",
      "    0.0            6246           2          3123.0            2620            3626  fread                                                                           \n",
      "    0.0            5588           1          5588.0            5588            5588  connect                                                                         \n",
      "    0.0            3216           1          3216.0            3216            3216  fcntl                                                                           \n",
      "    0.0            1971           1          1971.0            1971            1971  bind                                                                            \n",
      "    0.0            1331           1          1331.0            1331            1331  listen                                                                          \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Report file moved to \"/dli/task/report8.qdrep\"\r\n",
      "Report file moved to \"/dli/task/report8.sqlite\"\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocting memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications.\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vS0-BCGiWUb82r1RH-4cSRmZjN2vjebqoodlHIN1fvtt1iDh8X8W9WOSlLVxcsY747WVIebw13cDYBO/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vS0-BCGiWUb82r1RH-4cSRmZjN2vjebqoodlHIN1fvtt1iDh8X8W9WOSlLVxcsY747WVIebw13cDYBO/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Curently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the codebase, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occuring, with small memory migrations occuring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-6c32-ffce-7538-f91d.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6c32-ffce-7538-f91d.qdrep\"\n",
      "Exporting 1736 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6c32-ffce-7538-f91d.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   87.5       200868479           1     200868479.0       200868479       200868479  cudaMallocManaged                                                               \n",
      "    8.9        20467922           1      20467922.0        20467922        20467922  cudaDeviceSynchronize                                                           \n",
      "    3.6         8266097           1       8266097.0         8266097         8266097  cudaFree                                                                        \n",
      "    0.0           42173           1         42173.0           42173           42173  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0        20460368           1      20460368.0        20460368        20460368  deviceKernel(int*, int)                                                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21171520         768         27567.1            1695          159936  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   46.5       493923236          32      15435101.1           11870       100070644  sem_timedwait                                                                   \n",
      "   45.3       481490451          32      15046576.6           21505       100128629  poll                                                                            \n",
      "    7.1        75475828         578        130581.0            1093        15678097  ioctl                                                                           \n",
      "    1.0        10288220          84        122478.8            1164         8126100  mmap                                                                            \n",
      "    0.1          554621          77          7202.9            2387           12989  open64                                                                          \n",
      "    0.0          101170           4         25292.5           22367           29648  pthread_create                                                                  \n",
      "    0.0           93119           3         31039.7           23832           44964  fgets                                                                           \n",
      "    0.0           92257          23          4011.2            1274           16778  fopen                                                                           \n",
      "    0.0           76882          11          6989.3            3743           11305  write                                                                           \n",
      "    0.0           32215          11          2928.6            1399            4567  munmap                                                                          \n",
      "    0.0           27952           5          5590.4            2942            8274  open                                                                            \n",
      "    0.0           27809          16          1738.1            1042            3394  fclose                                                                          \n",
      "    0.0           23864          12          1988.7            1005            3802  read                                                                            \n",
      "    0.0           11497           3          3832.3            3630            4198  pipe2                                                                           \n",
      "    0.0            7386           2          3693.0            3668            3718  socket                                                                          \n",
      "    0.0            6129           2          3064.5            2623            3506  fread                                                                           \n",
      "    0.0            6040           4          1510.0            1428            1584  mprotect                                                                        \n",
      "    0.0            6015           3          2005.0            1072            3658  fcntl                                                                           \n",
      "    0.0            4988           1          4988.0            4988            4988  connect                                                                         \n",
      "    0.0            2363           1          2363.0            2363            2363  bind                                                                            \n",
      "    0.0            1291           1          1291.0            1291            1291  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report9.qdrep\"\n",
      "Report file moved to \"/dli/task/report9.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the codebase in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-be82-09e2-db2a-edd7.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-be82-09e2-db2a-edd7.qdrep\"\n",
      "Exporting 4228 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-be82-09e2-db2a-edd7.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   74.0       626737746           1     626737746.0       626737746       626737746  cudaDeviceSynchronize                                                           \n",
      "   23.8       201060127           3      67020042.3           18803       200980407  cudaMallocManaged                                                               \n",
      "    2.2        18665332           3       6221777.3         5545549         7307672  cudaFree                                                                        \n",
      "    0.0           51323           1         51323.0           51323           51323  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0       626726165           1     626726165.0       626726165       626726165  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.7        68525664        2304         29742.0            1503          183360  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.3        20855104         768         27155.1            1119          159648  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000            2304              170.667              4.000             1020.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   56.5      2127763985         115      18502295.5           19547       100127737  poll                                                                            \n",
      "   40.9      1542312188         115      13411410.3           11140       100072617  sem_timedwait                                                                   \n",
      "    2.0        75143168         587        128012.2            1044        15745564  ioctl                                                                           \n",
      "    0.5        20521907          90        228021.2            1054         7235187  mmap                                                                            \n",
      "    0.0          585463          77          7603.4            2317           25391  open64                                                                          \n",
      "    0.0          107958           4         26989.5           23710           30600  pthread_create                                                                  \n",
      "    0.0          100091          23          4351.8            1256           18654  fopen                                                                           \n",
      "    0.0           89025           3         29675.0           21560           44022  fgets                                                                           \n",
      "    0.0           78060          11          7096.4            4077           13500  write                                                                           \n",
      "    0.0           49227          16          3076.7            1468            6138  munmap                                                                          \n",
      "    0.0           32499           5          6499.8            2617            9071  open                                                                            \n",
      "    0.0           28161          16          1760.1            1049            3766  fclose                                                                          \n",
      "    0.0           23528          11          2138.9            1035            3707  read                                                                            \n",
      "    0.0           11759           3          3919.7            3099            5053  pipe2                                                                           \n",
      "    0.0            9643           2          4821.5            4165            5478  socket                                                                          \n",
      "    0.0            6821           2          3410.5            3100            3721  fread                                                                           \n",
      "    0.0            6198           4          1549.5            1428            1732  mprotect                                                                        \n",
      "    0.0            5455           1          5455.0            5455            5455  connect                                                                         \n",
      "    0.0            5308           2          2654.0            1050            4258  fcntl                                                                           \n",
      "    0.0            1880           1          1880.0            1880            1880  bind                                                                            \n",
      "    0.0            1417           1          1417.0            1417            1417  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report10.qdrep\"\n",
      "Report file moved to \"/dli/task/report10.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-d83a-6687-661b-2320.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-d83a-6687-661b-2320.qdrep\"\n",
      "Exporting 1770 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-d83a-6687-661b-2320.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   73.1       202714427           3      67571475.7           20024       202635165  cudaMallocManaged                                                               \n",
      "   19.4        53818083           2      26909041.5         1710341        52107742  cudaDeviceSynchronize                                                           \n",
      "    7.4        20656618           3       6885539.3         6243941         8045581  cudaFree                                                                        \n",
      "    0.0           88272           4         22068.0            5608           37080  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "   96.8        52106073           3      17368691.0        17203987        17670514  initWith(float, float*, int)                                                                                                                                                                                                                                                                                                                 \n",
      "    3.2         1709724           1       1709724.0         1709724         1709724  addArraysInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                   \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21160480         768         27552.7            1663          161376  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   48.7       591556533          34      17398721.6           18088       100135074  poll                                                                            \n",
      "   43.1       522852259          33      15844007.8           10203       100071503  sem_timedwait                                                                   \n",
      "    6.3        75902556         587        129305.9            1043        15760403  ioctl                                                                           \n",
      "    1.9        22553927          89        253414.9            1116         7988355  mmap                                                                            \n",
      "    0.0          579389          77          7524.5            2213           16803  open64                                                                          \n",
      "    0.0          117671           4         29417.8           22925           36531  pthread_create                                                                  \n",
      "    0.0          106260          23          4620.0            1589           18542  fopen                                                                           \n",
      "    0.0           91025           3         30341.7           23408           43569  fgets                                                                           \n",
      "    0.0           77977          11          7088.8            3319           12061  write                                                                           \n",
      "    0.0           46640          16          2915.0            1502            5370  munmap                                                                          \n",
      "    0.0           32720           5          6544.0            3106            8413  open                                                                            \n",
      "    0.0           27873          15          1858.2            1056            3683  fclose                                                                          \n",
      "    0.0           25370          12          2114.2            1021            3316  read                                                                            \n",
      "    0.0           12918           2          6459.0            4384            8534  socket                                                                          \n",
      "    0.0           11447           3          3815.7            3524            4058  pipe2                                                                           \n",
      "    0.0            8971           3          2990.3            1544            4181  fread                                                                           \n",
      "    0.0            8454           1          8454.0            8454            8454  connect                                                                         \n",
      "    0.0            6770           4          1692.5            1575            1964  mprotect                                                                        \n",
      "    0.0            6161           3          2053.7            1098            3920  fcntl                                                                           \n",
      "    0.0            2040           1          2040.0            2040            2040  bind                                                                            \n",
      "    0.0            1400           1          1400.0            1400            1400  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report11.qdrep\"\n",
      "Report file moved to \"/dli/task/report11.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](../edit/08-prefetch/01-vector-add-prefetch.cu), and update your own codebase to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specificially, as well as the impact on the reported run time of the initialization kernel, before each experiement, and then verify by running `nsys profile`. Refer to [the solution](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-a7ca-bf04-4ec7-ae8c.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-a7ca-bf04-4ec7-ae8c.qdrep\"\n",
      "Exporting 2033 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-a7ca-bf04-4ec7-ae8c.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   69.2       201077193           3      67025731.0           22778       201005080  cudaMallocManaged                                                               \n",
      "   20.7        60175072           1      60175072.0        60175072        60175072  cudaDeviceSynchronize                                                           \n",
      "    6.7        19496420           3       6498806.7         5452124         8410392  cudaFree                                                                        \n",
      "    3.3         9586667           3       3195555.7            6409         9466328  cudaMemPrefetchAsync                                                            \n",
      "    0.0           46103           1         46103.0           46103           46103  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0         1706971           1       1706971.0         1706971         1706971  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   75.7        65795584         192        342685.3          339872          354144  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   24.3        21153344         768         27543.4            1663          159840  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000             192             2048.000           2048.000             2048.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000             768              170.667              4.000             1020.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.5      1174907332          69      17027642.5           16010       100132131  poll                                                                            \n",
      "   40.7       909504103          64      14211001.6            9645       100064169  sem_timedwait                                                                   \n",
      "    5.2       116970630         591        197919.8            1109        22619444  ioctl                                                                           \n",
      "    1.0        21439651          91        235600.6            1102         8350923  mmap                                                                            \n",
      "    0.6        13289426           3       4429808.7           35886        13145592  sem_wait                                                                        \n",
      "    0.0          573117          77          7443.1            2241           13161  open64                                                                          \n",
      "    0.0          157532           5         31506.4           23560           42950  pthread_create                                                                  \n",
      "    0.0          101150           3         33716.7           23300           44735  fgets                                                                           \n",
      "    0.0           95585          23          4155.9            1191           12631  fopen                                                                           \n",
      "    0.0           91425          13          7032.7            3693           11954  write                                                                           \n",
      "    0.0           70342          16          4396.4            1047           27650  fclose                                                                          \n",
      "    0.0           38644          13          2972.6            1377            5380  munmap                                                                          \n",
      "    0.0           30314           5          6062.8            2759            9261  open                                                                            \n",
      "    0.0           29992          13          2307.1            1073            4146  read                                                                            \n",
      "    0.0           11234           3          3744.7            3676            3808  pipe2                                                                           \n",
      "    0.0           10463           5          2092.6            1573            2975  mprotect                                                                        \n",
      "    0.0            8100           2          4050.0            3965            4135  socket                                                                          \n",
      "    0.0            5879           2          2939.5            2465            3414  fread                                                                           \n",
      "    0.0            5019           1          5019.0            5019            5019  connect                                                                         \n",
      "    0.0            4884           2          2442.0            1082            3802  fcntl                                                                           \n",
      "    0.0            1984           1          1984.0            1984            1984  bind                                                                            \n",
      "    0.0            1319           1          1319.0            1319            1319  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report12.qdrep\"\n",
      "Report file moved to \"/dli/task/report12.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Device ID: 0\tNumber of SMs: 40\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-7094-6355-ddbc-a39e.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-7094-6355-ddbc-a39e.qdrep\"\n",
      "Exporting 1327 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-7094-6355-ddbc-a39e.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   49.7       206628065           3      68876021.7           18325       206568738  cudaMallocManaged                                                               \n",
      "   31.3       130186716           7      18598102.3            7627        33388548  cudaMemPrefetchAsync                                                            \n",
      "   14.5        60114768           1      60114768.0        60114768        60114768  cudaDeviceSynchronize                                                           \n",
      "    4.5        18492994           3       6164331.3         5475760         7470264  cudaFree                                                                        \n",
      "    0.0           41663           1         41663.0           41663           41663  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0         1698587           1       1698587.0         1698587         1698587  addVectorsInto(float*, float*, float*, int)                                                                                                                                                                                                                                                                                                  \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.4        65808064         192        342750.3          339840          355808  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   23.6        20349472          64        317960.5          310976          324000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "         393216.000             192             2048.000           2048.000             2048.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.000              64             2048.000           2048.000             2048.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   47.9      1024108589          63      16255691.9           16931       100142856  poll                                                                            \n",
      "   39.1       834718265          59      14147767.2           11140       100074649  sem_timedwait                                                                   \n",
      "   11.4       242792287         595        408054.3            1039        33339515  ioctl                                                                           \n",
      "    1.0        20384271          91        224003.0            1123         7404945  mmap                                                                            \n",
      "    0.6        13162482           3       4387494.0           30565        13023035  sem_wait                                                                        \n",
      "    0.0          583275          77          7575.0            2228           12598  open64                                                                          \n",
      "    0.0          142296           5         28459.2           22029           42415  pthread_create                                                                  \n",
      "    0.0           95313           3         31771.0           23296           42544  fgets                                                                           \n",
      "    0.0           88049          23          3828.2            1130           12314  fopen                                                                           \n",
      "    0.0           81598          13          6276.8            3246            9655  write                                                                           \n",
      "    0.0           40196          14          2871.1            1246            4850  munmap                                                                          \n",
      "    0.0           27579          14          1969.9            1050            2848  read                                                                            \n",
      "    0.0           26998          16          1687.4            1043            2878  fclose                                                                          \n",
      "    0.0           26503           5          5300.6            2513            7582  open                                                                            \n",
      "    0.0           26025           3          8675.0            2752           20014  pipe2                                                                           \n",
      "    0.0            8419           5          1683.8            1257            2282  mprotect                                                                        \n",
      "    0.0            6977           2          3488.5            3247            3730  socket                                                                          \n",
      "    0.0            5379           2          2689.5            2168            3211  fread                                                                           \n",
      "    0.0            4164           1          4164.0            4164            4164  connect                                                                         \n",
      "    0.0            3883           1          3883.0            3883            3883  fcntl                                                                           \n",
      "    0.0            1972           1          1972.0            1972            1972  bind                                                                            \n",
      "    0.0            1263           1          1263.0            1263            1263  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report13.qdrep\"\n",
      "Report file moved to \"/dli/task/report13.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated [SAXPY](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_1) application has been provided for you [here](../edit/09-saxpy/01-saxpy.cu). It currently contains a couple of bugs that you will need to find and fix before you can successfully compile, run, and then profile it with `nsys profile`.\n",
    "\n",
    "After fixing the bugs and profiling the application, record the runtime of the `saxpy` kernel and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *100us*. Check out [the solution](../edit/09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \r\n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-7899-db8c-3941-dddb.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-7899-db8c-3941-dddb.qdrep\"\n",
      "Exporting 1002 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-7899-db8c-3941-dddb.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   94.3       202140427           3      67380142.3           24204       202091087  cudaMallocManaged                                                               \n",
      "    3.8         8178992           1       8178992.0         8178992         8178992  cudaDeviceSynchronize                                                           \n",
      "    1.1         2377431           3        792477.0          768930          832658  cudaFree                                                                        \n",
      "    0.8         1639765           3        546588.3            7913         1509734  cudaMemPrefetchAsync                                                            \n",
      "    0.0           43873           1         43873.0           43873           43873  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                                                                                                                                                                                                                                                                                         \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------------------------------------------                                                                                                                                                                                                                         \n",
      "  100.0          199265           1        199265.0          199265          199265  saxpy(int*, int*, int*)                                                                                                                                                                                                                                                                                                                      \n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.7         8229088          24        342878.7          340000          355648  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "    0.3           24382           4          6095.5            1759           10335  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "              Total      Operations              Average            Minimum              Maximum  Name                                                                            \n",
      "-------------------  --------------  -------------------  -----------------  -------------------  --------------------------------------------------------------------------------\n",
      "          49152.000              24             2048.000           2048.000             2048.000  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "            128.000               4               32.000              4.000               60.000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   45.8       337084900          17      19828523.5            9960       100061147  sem_timedwait                                                                   \n",
      "   42.0       309211470          24      12883811.3           20339       100115807  poll                                                                            \n",
      "   10.9        80333149         589        136389.0            1018        15731638  ioctl                                                                           \n",
      "    0.6         4389145          89         49316.2            1117          740603  mmap                                                                            \n",
      "    0.5         3608519           3       1202839.7           40189         1812091  sem_wait                                                                        \n",
      "    0.1          636603          77          8267.6            2744           27147  open64                                                                          \n",
      "    0.0          150158           5         30031.6           22991           47179  pthread_create                                                                  \n",
      "    0.0          126694          13          9745.7            4017           24376  write                                                                           \n",
      "    0.0           92878          23          4038.2            1162           13647  fopen                                                                           \n",
      "    0.0           92257           3         30752.3           23229           44829  fgets                                                                           \n",
      "    0.0           42900           5          8580.0            3007           19897  open                                                                            \n",
      "    0.0           29685          13          2283.5            1133            3497  read                                                                            \n",
      "    0.0           29222          11          2656.5            1214            3849  munmap                                                                          \n",
      "    0.0           26701          16          1668.8            1024            2729  fclose                                                                          \n",
      "    0.0           12772           3          4257.3            3725            4842  pipe2                                                                           \n",
      "    0.0           10289           2          5144.5            3626            6663  socket                                                                          \n",
      "    0.0            8783           5          1756.6            1444            2276  mprotect                                                                        \n",
      "    0.0            6134           2          3067.0            2208            3926  fread                                                                           \n",
      "    0.0            6006           1          6006.0            6006            6006  connect                                                                         \n",
      "    0.0            4018           1          4018.0            4018            4018  fcntl                                                                           \n",
      "    0.0            2053           1          2053.0            2053            2053  bind                                                                            \n",
      "    0.0            1308           1          1308.0            1308            1308  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Report file moved to \"/dli/task/report14.qdrep\"\n",
      "Report file moved to \"/dli/task/report14.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
